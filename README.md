README

LLama model: ollama run llama3
AI runs at:
http://localhost:11434

Then
./mvnw spring-boot:run
Server runs at:
http://localhost:8080
