README

LLama model: ollama run llama3
AI runs at:
http://localhost:11434

Then
./mvnw spring-boot:run
Server runs at:
http://localhost:8080

Demo Video:
https://youtu.be/YktMPQ50b_Q
